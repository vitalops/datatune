{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d96f064",
   "metadata": {},
   "source": [
    "# Datatune with Semantic Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bfaa5",
   "metadata": {},
   "source": [
    "Let's start by installing dependencies. We will be using duckdb as our database backend and OpenAI LLM API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datatune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b326c27",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatune as dt\n",
    "import seaborn as sns\n",
    "from datatune.llm.llm import OpenAI\n",
    "import dask.dataframe as dd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d36ed",
   "metadata": {},
   "source": [
    "## Initialize your LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26156d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"     # Replace with your actual OpenAI API key\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", rpm=500, tpm=150000)      # Initialize the LLM with your rate limits                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddba029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"flights\")      # Load the flights dataset\n",
    "df = dd.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda49b6",
   "metadata": {},
   "source": [
    "## Get deduplication clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41090e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = dt.reduce(df, action=\"dedup\", embedding_model=\"text-embedding-3-small\", llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d65000",
   "metadata": {},
   "source": [
    "Reduce gets a deduplication map that can be passed to map and filter which sends only canonical rows to the LLM API for transformation and transmits their result to the duplicate rows thereby reducing tokens and therfore cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c327ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping and filtering prompt to transform the dataset\n",
    "\n",
    "mapping_prompt = \"Add a column passenger_trend_comment that describes the trend in passenger numbers..make the comment descriptive and varied\"\n",
    "\n",
    "filtering_prompt = \"Based on the passenger_trend_comment column, filter the dataset to include only those months where there is a significant change in passenger numbers compared to the previous month.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4ee14",
   "metadata": {},
   "source": [
    "## Transforming our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb992c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = dt.map(\n",
    "    prompt = mapping_prompt,\n",
    "    output_fields=[\"passenger_trend_comment\"],       # input fields to be used for mapping\n",
    "    input_fields=[\"passengers\",\"month\",\"year\"],\n",
    "    clusters=clusters                               # pass deduplication clusters\n",
    ")(llm, df)\n",
    "\n",
    "# Now pass the mapped Ibis table expression to filter\n",
    "filtered = dt.filter(\n",
    "    prompt = filtering_prompt,\n",
    "    input_fields=[\"passenger_trend_comment\"],\n",
    "    clusters=clusters                               # pass deduplication clusters\n",
    ")(llm, mapped)\n",
    "\n",
    "result = filtered.execute()      # Result is a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f59d3",
   "metadata": {},
   "source": [
    "## Convert the transformed dataset into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"duckdb_transformed.csv\")\n",
    "\n",
    "print(result.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
